---
title: "xgboost"
author: "Keerthana"
output: html_document
---
### This file appiles Extreme Gradient Boosting to the dataset
```{r}
library(caret)
library(class)
library(ISLR)
# loading training data
data_train <- read.table("train_data.txt",sep=",",row.names=NULL,col.names=c("subject id", "jitter1","jitter2","jitter3","jitter4","jitter5","shimmer1","shimmer2","shimmer3","shimmer4","shimmer5","shimmer6","AC","NTH","HTN","median pitch","mean pitch","sd","min pitch","max pitch","num pulses","Number of periods","Mean period","Standard deviation of period","Fraction of locally unvoiced frames","Number of voice breaks","Degree of voice breaks","UPDRS","class"),fill=FALSE)
train_data=data_train[,1:27]
train_response=data_train[,29]

# loading test data
data_test <- read.table("test_data.txt",sep=",",row.names=NULL,col.names=c("subject id", "jitter1","jitter2","jitter3","jitter4","jitter5","shimmer1","shimmer2","shimmer3","shimmer4","shimmer5","shimmer6","AC","NTH","HTN","median pitch","mean pitch","sd","min pitch","max pitch","num pulses","Number of periods","Mean period","Standard deviation of period","Fraction of locally unvoiced frames","Number of voice breaks","Degree of voice breaks","class"),fill=FALSE)
test_data=data_test[,1:27]
test_response=data_test[,28]

```

### Combining training and testing data

```{r}
train_data_norm=train_data[,2:27]
test_data_norm=test_data[,2:27]
data=rbind(train_data_norm,test_data_norm)
# Uncomment the following line of code to use only 14 features to train and test the model
#data=data[,c(2,6,10,12,13,14,17,18,19,20,21,24,25,26)]
response=c(train_response,test_response)
```

### Random Forest applied to the entire dataset

```{r}
# xboost
library(xgboost)
library(readr)
library(stringr)
library(caret)

# Matrices to store performance metrics for each run
acc=matrix(0,10,1)
MCC=matrix(0,10,1)
sensi=matrix(0,10,1)
speci=matrix(0,10,1)
for (k in 1:10){
set.seed(k)
Rand_index<-sample(1:1208, 800) 
# generate training and testing data from the original data
X_train=scale(data[Rand_index,])
X_test=scale(data[-Rand_index,])
y_train=response[Rand_index]
y_test=response[-Rand_index]

# Creating training and testing Matrices
dtrain <- xgb.DMatrix(data = X_train, label=as.numeric(y_train))
dtest<-xgb.DMatrix(data = X_test, label=as.numeric(y_test))
watchlist <- list(train=dtrain, test=dtest)

# Training using XGboost algorithm
bst <- xgb.train(data=dtrain, max_depth=3, eta=1, nthread = 2, nrounds=2, watchlist=watchlist, objective = "binary:logistic")

# prediction
pred<-predict(bst,X_test)
pred<- as.numeric(pred > 0.5)
TN=0
TP=0
FN=0
FP=0
for (i in 1:408){
  if(y_test[i]==pred[i])
  {
    if(pred[i]==0)
      TN=TN+1
    else
      TP=TP+1
  }
  else
  {
    if(pred[i]==0)
      FN=FN+1
    else
      FP=FP+1
  }
}
acc[k]=(TP+TN)/408
sensi[k]=TP/(TP+FN)
speci[k]=TN/(TN+FP)
MCC[k]=(TP*TN-FP*FN)/sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))
}
mean(acc)
mean(sensi)
mean(speci)
mean(MCC)
```

### Summarize the dataset using central tendency and dispersion metrics

```{r}
train_mat1=matrix(0,40,28)
train_mat2=matrix(0,40,28)
train_mat3=matrix(0,40,28)
train_response_mat=matrix(0,40,1)
for (i in 0:39){
for (j in 1:14){
l=i*26+1
m=i*26+26
train_mat1[i+1,j]=mean(train_data_norm[l:m,j])
train_mat2[i+1,j]=median(train_data_norm[l:m,j])
train_mat3[i+1,j]=mean(train_data_norm[l:m,j],trim = 0.125)

train_mat1[i+1,j+14]=sd(train_data_norm[l:m,j])
train_mat2[i+1,j+14]=mad(train_data_norm[l:m,j])
train_mat3[i+1,j+14]=IQR(train_data_norm[l:m,j])

}
train_response_mat[i+1]=train_response[i*26+1]
}

test_mat1=matrix(0,28,28)
test_mat2=matrix(0,28,28)
test_mat3=matrix(0,28,28)
test_response_mat=matrix(0,28,1)
for (i in 0:27){
for (j in 1:14){
l=i*6+1
m=i*6+6
test_mat1[i+1,j]=mean(test_data_norm[l:m,j])
test_mat2[i+1,j]=median(test_data_norm[l:m,j])
test_mat3[i+1,j]=mean(test_data_norm[l:m,j],trim = 0.125)

test_mat1[i+1,j+14]=sd(test_data_norm[l:m,j])
test_mat2[i+1,j+14]=mad(test_data_norm[l:m,j])
test_mat3[i+1,j+14]=IQR(test_data_norm[l:m,j])
}
  test_response_mat[i+1]=test_response[i*6+1]
}
train_mat=cbind(train_mat1,train_mat2,train_mat3)
test_mat=cbind(test_mat1,test_mat2,test_mat3)
data_modified1=rbind(train_mat1,test_mat1)
data_modified2=rbind(train_mat2,test_mat2)
data_modified3=rbind(train_mat3,test_mat3)
data_modified=rbind(train_mat,test_mat)
modified_response=c(train_response_mat,test_response_mat)


```

### XGBoost applied to summarized datasets

```{r}
# xboost
library(xgboost)
library(readr)
library(stringr)
library(caret)
# Matrices to store performance metrics for each run
acc=matrix(0,10,1)
MCC=matrix(0,10,1)
sensi=matrix(0,10,1)
speci=matrix(0,10,1)
for (k in 1:10){
set.seed(k)
Rand_index<-sample(1:68, 40) 

# generate training and testing data from the original data
X_train=scale(data_modified2[Rand_index,])
X_test=scale(data_modified2[-Rand_index,])
y_train=modified_response[Rand_index]
y_test=modified_response[-Rand_index]

# Creating training and testing Matrices
dtrain <- xgb.DMatrix(data = X_train, label=as.numeric(y_train))
dtest<-xgb.DMatrix(data = X_test, label=as.numeric(y_test))
watchlist <- list(train=dtrain, test=dtest)

# Training using XGboost algorithm
bst <- xgb.train(data=dtrain, max_depth=3, eta=1, nthread = 2, nrounds=2, watchlist=watchlist, objective = "binary:logistic")

# prediction
pred<-predict(bst,X_test)
pred<- as.numeric(pred > 0.5)
TN=0
TP=0
FN=0
FP=0
for (i in 1:28){
  if(y_test[i]==pred[i])
  {
    if(pred[i]==0)
      TN=TN+1
    else
      TP=TP+1
  }
  else
  {
    if(pred[i]==0)
      FN=FN+1
    else
      FP=FP+1
  }
}
acc[k]=(TP+TN)/28
sensi[k]=TP/(TP+FN)
speci[k]=TN/(TN+FP)
MCC[k]=(TP*TN-FP*FN)/sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))
}
mean(acc)
mean(sensi)
mean(speci)
mean(MCC)
```